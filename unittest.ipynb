{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  # `unittest`\n",
    "**Testing with the standard library**\n",
    "\n",
    "[docs.python.org/3/library/unittest.html](http://docs.python.org/3/library/unittest.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic features\n",
    "\n",
    "* Test automation\n",
    "* Sharing of setup and shutdown code for tests\n",
    "* Aggregation of tests into collections\n",
    "* Independence of the tests from the reporting framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* TestCase the basic unit of test organization\n",
    " * provides assertions\n",
    " * provides fixtures\n",
    " * Groups together related test functions\n",
    "* Test methods\n",
    "* assertions\n",
    " * equality, truth, is/not\n",
    " * contained\n",
    " * raises, logs, warns\n",
    " * comparison operators\n",
    " * list/set/etc. equality\n",
    "* fixtures\n",
    " * setUp/tearDown\n",
    " * setUpClass/tearDownClass\n",
    " * setUpModule/tearDownModule\n",
    "* test discovery\n",
    "* unittest.main()\n",
    "* test skipping\n",
    "* subtests\n",
    "* TestLoader\n",
    "* TestRunner\n",
    "* TestResult\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `unittest.TestCase`\n",
    "*The basic unit of test organization*\n",
    "\n",
    "* provides assertion methods\n",
    "* test and class fixtures\n",
    "* related test methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**To create a test**, subclass from `unittest.TestCase` and implement test methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import importlib\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "__file__ = 'unittest.ipynb'\n",
    "\n",
    "def format_result(result, report_expected_failures):\n",
    "    data = []\n",
    "    details = []\n",
    "    data.append('FAILURES: {}'.format(len(result.failures)))\n",
    "    for f in result.failures:\n",
    "        details.append(f[0])\n",
    "    data.append('ERRORS: {}'.format(len(result.errors)))\n",
    "    for e in result.errors:\n",
    "        details.append(e[0])\n",
    "    data.append('SKIPPED: {}'.format(len(result.skipped)))\n",
    "    for s in result.skipped:\n",
    "        details.append(s[0])\n",
    "    if report_expected_failures:\n",
    "        data.append('EXPECTED FAILURES: {}'.format(len(result.expectedFailures)))\n",
    "        for f in result.expectedFailures:\n",
    "            details.append(f[0])\n",
    "        data.append('UNEXPECTED SUCCESSES: {}'.format(len(result.unexpectedSuccesses)))\n",
    "        for s in result.unexpectedSuccesses:\n",
    "            details.append(s)\n",
    "    data.append('TOTAL: {}'.format(result.testsRun))\n",
    "    \n",
    "    print(' '.join(data))\n",
    "    for d in details:\n",
    "        print(d)\n",
    "        \n",
    "@register_cell_magic\n",
    "def run_unittest(_, cell):\n",
    "    \"run tests defined in a cell with unittest\"\n",
    "    node = ast.parse(cell, __file__, 'exec')\n",
    "    mod = importlib.types.ModuleType('__test__')\n",
    "    mod.__dict__['unittest'] = unittest\n",
    "    exec(compile(node, '__main__', 'exec'), mod.__dict__)\n",
    "    runner = unittest.defaultTestLoader.loadTestsFromModule(mod)\n",
    "    result = unittest.TestResult()\n",
    "    result = runner.run(result)\n",
    "    format_result(result, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The simplest `TestCase` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "# This TestCase does nothing\n",
    "class Tests(unittest.TestCase):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not terribly exciting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Test methods start with \"test\"\n",
    "\n",
    "Actual testing occurs in `TestCase` methods that start with \"test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # test_ indicates a test to be run. This one does nothing.\n",
    "    def test_empty(self):\n",
    "        pass\n",
    "    \n",
    "    # So does just \"test\", but it's less Pythonic\n",
    "    def testNothing(self):\n",
    "        pass\n",
    "    \n",
    "    # This test isn't found because it doesn't start with test\n",
    "    def empty_test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Still not very interesting, but we see that a test has been run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assertions\n",
    "*Expressions of expectations*\n",
    "\n",
    "`TestCase` provides assertion methods that determine if tests pass or fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # This test passes trivially\n",
    "    def test_always_passes(self):\n",
    "        self.assertTrue(True)\n",
    "        \n",
    "    # This test fails trivially\n",
    "    def test_always_fails(self):\n",
    "        self.assertFalse(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Common assertions\n",
    "\n",
    "| method name                 | checks               |\n",
    "|-----------------------------|----------------------|\n",
    "| `assertEqual(a, b)`         | `a == b`             | \n",
    "| `assertNotEqual(a, b)`      | `a != b`             | \n",
    "| `assertTrue(x)`\t          | `bool(x) is True`    | \n",
    "| `assertFalse(x)`            | `bool(x) is False`   | \n",
    "| `assertIs(a, b)`\t\t      | `a is b`             | \n",
    "| `assertIsNot(a, b)`\t\t  | `a is not b`         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| method name                | checks                 |\n",
    "|----------------------------|------------------------|\n",
    "| `assertIsNone(x)`\t         | `x is None`            |\n",
    "| `assertIsNotNone(x)`\t\t | `x is not None`        |\n",
    "| `assertIn(a, b)`\t\t     | `a in b`               |\n",
    "| `assertNotIn(a, b)`        | `a not in b`           |\n",
    "|`assertIsInstance(a, b)`\t | `isinstance(a, b)`     |\n",
    "|`assertNotIsInstance(a, b)` | `not isinstance(a, b)` |\n",
    "\n",
    "See full details in the [`TestCase` documentation](https://docs.python.org/3/library/unittest.html#unittest.TestCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing for exceptions\n",
    "`assertRaises` is a *context-manager* that checks for an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # Verify that an IndexError is thrown in the test\n",
    "    def test_index_error(self):\n",
    "        with self.assertRaises(IndexError):\n",
    "            raise IndexError()\n",
    "            \n",
    "    def test_value_error(self):\n",
    "        # Verify that a ValueError is thrown in the test\n",
    "        with self.assertRaises(ValueError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fixtures\n",
    "*Test set-up and clean-up*\n",
    "\n",
    "Code run at various times in the test run:\n",
    "\n",
    "| name                     | type            | when                         |\n",
    "|--------------------------|-----------------|------------------------------|\n",
    "| `TestCase.setUp`         | instance method | before each test method      |\n",
    "| `TestCase.tearDown`      | instance method | after each test method       |\n",
    "| `TestCase.setUpClass`    | class method    | before any methods run       |\n",
    "| `TestCase.tearDownClass` | class method    | after all methods run        |\n",
    "| `setUpModule`            | global function | before any methods in module |\n",
    "| `tearDownModule`         | global function | after all methods in module  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What are fixtures for?\n",
    "They ensure a known state for tests and clean up resources.\n",
    "\n",
    "* Create/clear database tables\n",
    "* Configure instance members\n",
    "* Create/delete temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # This is run once before any of the tests in the TestCase\n",
    "    @classmethod\n",
    "    def setUpClass(cls): print('set up class')\n",
    "        \n",
    "    # This is run once after all of the tests in the TestCase have run\n",
    "    @classmethod\n",
    "    def tearDownClass(cls): print('tear down class')\n",
    "        \n",
    "    # This is run before *each* test in the TestCase\n",
    "    def setUp(self): print('- set up')\n",
    "        \n",
    "    # This is run after *each* test in the TestCase\n",
    "    def tearDown(self): print('- tear down')\n",
    "        \n",
    "    # A few demo tests\n",
    "    def test_one(self): print('-- test one')\n",
    "    def test_two(self): print('-- test two')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Configuring member attributes\n",
    "\n",
    "`setUp()` is often used to assign to attributes used in tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # self.data will be set to this value before each test in the TestCase\n",
    "        self.data = {'a': 2}\n",
    "        \n",
    "    def test_foo(self):\n",
    "        # We can test the value of self.data\n",
    "        self.assertEqual(self.data['a'], 2)\n",
    "        \n",
    "        # Modifications to self.data will be reset by setUp()\n",
    "        self.data['a'] = 42\n",
    "        \n",
    "    def test_bar(self):\n",
    "        self.assertEqual(self.data['a'], 2)\n",
    "        self.data['a'] = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `unittest.main()`\n",
    "*Convenience function for running from the command line*\n",
    "\n",
    "\"Discovers\" all available tests and executes them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "import unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # This fails trivially\n",
    "    def test_foo(self):\n",
    "        self.assertEqual(1, 2)\n",
    "\n",
    "# This the idiomatic test for \"am I the top-level program\" in Python\n",
    "if __name__ == '__main__':\n",
    "    # If this is the top-level program, this scans for all TestCases and runs them.\n",
    "    unittest.main()\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%run examples/unittest_main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skipping tests with decorators\n",
    "*Use the decorators `skip()`, `skipIf`, and `skipUnless` to skip tests*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "# This controls which tests run\n",
    "some_condition = True\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    @unittest.skip('This test never runs')\n",
    "    def test_no_run(self):\n",
    "        print('This will never execute in a test suite.')\n",
    "        \n",
    "    # We only run this test if our condition is False\n",
    "    @unittest.skipIf(some_condition == True, 'Condition is true')\n",
    "    def test_when_condition_is_false(self):\n",
    "        pass\n",
    "    \n",
    "    # We only run this test if our condition is True\n",
    "    @unittest.skipUnless(some_condition == True, 'Condition is false')\n",
    "    def test_when_condition_is_true(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skipping tests from fixtures with `SkipTest`\n",
    "* Throw the `SkipTest` exception from `setUp()` to skip tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # We skip tests if a necessary resource is not available\n",
    "        raise unittest.SkipTest(\n",
    "            'Satellite uplink is not available right now.')\n",
    "        \n",
    "    # This test will only be run if the satellite uplink is available\n",
    "    def test_contact_satellite(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Allow failures with `expectedFailure`\n",
    "*`expectedFailure` allows a failing test to count as passing*\n",
    "\n",
    "This can be useful for \"reminder\" tests or work-in-progress. For example, it can be used to alert you when a broken upstream dependency has started working.\n",
    "\n",
    "The command-line runner will let you know when there are expected failures *and* when there are unexpected successes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # The failing assertion in this test is not counted as a failure\n",
    "    @unittest.expectedFailure\n",
    "    def test_this_will_fail(self):\n",
    "        self.assertTrue(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameterize tests with `TestCase.subTest`\n",
    "*Run the same test with multiple parameters, treating each as a separate test*\n",
    "*New in 3.4*\n",
    "\n",
    "This reduces boilerplate in certain situations since failure of one subtest does not prevent execution of other subtests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%run_unittest\n",
    "\n",
    "import string\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    def test_all_ascii(self):\n",
    "        for word in ['thread', 'tråd', 'island', 'øy']:\n",
    "            # Each subtest context is treated as an independent test\n",
    "            with self.subTest(word=word):\n",
    "                self.assertTrue(all(c in string.ascii_lowercase for c in word))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "theme": "white",
   "transition": "slide"
  },
  "name": "unittest.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
