{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"theme/sixty_north.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"theme/sixty_north.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%run unittest_magics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `unittest`\n",
    "## Testing with the standard library\n",
    "\n",
    "[docs.python.org/3/library/unittest.html](http://docs.python.org/3/library/unittest.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic features\n",
    "\n",
    "* Test automation\n",
    "* Sharing of setup and shutdown code for tests\n",
    "* Aggregation of tests into collections\n",
    "* Independence of the tests from the reporting framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `unittest.TestCase`\n",
    "## The basic unit of test organization\n",
    "\n",
    "* provides assertion methods\n",
    "* test and class fixtures\n",
    "* related test methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**To create a test**, subclass from `unittest.TestCase` and implement test methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The simplest `TestCase` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 0 ERRORS: 0 SKIPPED: 0 TOTAL: 0\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "# This TestCase does nothing\n",
    "class Tests(unittest.TestCase):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not terribly exciting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Test methods start with \"test\"\n",
    "\n",
    "Actual testing occurs in `TestCase` methods that start with \"test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 0 ERRORS: 0 SKIPPED: 0 TOTAL: 2\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # test_ indicates a test to be run. This one does nothing.\n",
    "    def test_empty(self):\n",
    "        pass\n",
    "    \n",
    "    # So does just \"test\", but it's less Pythonic\n",
    "    def testNothing(self):\n",
    "        pass\n",
    "    \n",
    "    # This test isn't found because it doesn't start with test\n",
    "    def empty_test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Still not very interesting, but we see that a test has been run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assertions\n",
    "## Expressions of expectations\n",
    "\n",
    "`TestCase` provides assertion methods that determine if tests pass or fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 1 ERRORS: 0 SKIPPED: 0 TOTAL: 2\n",
      "f: test_always_fails (__main__.Tests)\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # This test passes trivially\n",
    "    def test_always_passes(self):\n",
    "        self.assertTrue(True)\n",
    "        \n",
    "    # This test fails trivially\n",
    "    def test_always_fails(self):\n",
    "        self.assertFalse(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Common assertions\n",
    "\n",
    "| method name                 | checks               |\n",
    "|-----------------------------|----------------------|\n",
    "| `assertEqual(a, b)`         | `a == b`             | \n",
    "| `assertNotEqual(a, b)`      | `a != b`             | \n",
    "| `assertTrue(x)`\t          | `bool(x) is True`    | \n",
    "| `assertFalse(x)`            | `bool(x) is False`   | \n",
    "| `assertIs(a, b)`\t\t      | `a is b`             | \n",
    "| `assertIsNot(a, b)`\t\t  | `a is not b`         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| method name                | checks                 |\n",
    "|----------------------------|------------------------|\n",
    "| `assertIsNone(x)`\t         | `x is None`            |\n",
    "| `assertIsNotNone(x)`\t\t | `x is not None`        |\n",
    "| `assertIn(a, b)`\t\t     | `a in b`               |\n",
    "| `assertNotIn(a, b)`        | `a not in b`           |\n",
    "|`assertIsInstance(a, b)`\t | `isinstance(a, b)`     |\n",
    "|`assertNotIsInstance(a, b)` | `not isinstance(a, b)` |\n",
    "\n",
    "See full details in the [`TestCase` documentation](https://docs.python.org/3/library/unittest.html#unittest.TestCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Testing for exceptions\n",
    "`assertRaises` is a *context-manager* that checks for an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 1 ERRORS: 0 SKIPPED: 0 TOTAL: 2\n",
      "f: test_value_error (__main__.Tests)\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # Verify that an IndexError is thrown in the test\n",
    "    def test_index_error(self): \n",
    "        with self.assertRaises(IndexError):\n",
    "            raise IndexError()\n",
    "            \n",
    "    def test_value_error(self):\n",
    "        # Verify that a ValueError is thrown in the test\n",
    "        with self.assertRaises(ValueError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fixtures\n",
    "## Test set-up and clean-up\n",
    "\n",
    "Code run at various times in the test run:\n",
    "\n",
    "| name                     | type            | when                         |\n",
    "|--------------------------|-----------------|------------------------------|\n",
    "| `TestCase.setUp`         | instance method | before each test method      |\n",
    "| `TestCase.tearDown`      | instance method | after each test method       |\n",
    "| `TestCase.setUpClass`    | class method    | before any methods run       |\n",
    "| `TestCase.tearDownClass` | class method    | after all methods run        |\n",
    "| `setUpModule`            | global function | before any methods in module |\n",
    "| `tearDownModule`         | global function | after all methods in module  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What are fixtures for?\n",
    "They ensure a known state for tests and clean up resources.\n",
    "\n",
    "* Create/clear database tables\n",
    "* Configure instance members\n",
    "* Create/delete temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%unittest_run -n\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # This is run once before any of the tests in the TestCase\n",
    "    @classmethod\n",
    "    def setUpClass(cls): print('set up class')\n",
    "        \n",
    "    # This is run once after all of the tests in the TestCase have run\n",
    "    @classmethod\n",
    "    def tearDownClass(cls): print('tear down class')\n",
    "        \n",
    "    # This is run before *each* test in the TestCase\n",
    "    def setUp(self): print('- set up')\n",
    "        \n",
    "    # This is run after *each* test in the TestCase\n",
    "    def tearDown(self): print('- tear down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up class\n",
      "- set up\n",
      "-- test one\n",
      "- tear down\n",
      "- set up\n",
      "-- test two\n",
      "- tear down\n",
      "tear down class\n",
      "FAILURES: 0 ERRORS: 0 SKIPPED: 0 TOTAL: 2\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "    # A few demo tests\n",
    "    def test_one(self): print('-- test one')\n",
    "    def test_two(self): print('-- test two')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Configuring member attributes\n",
    "\n",
    "`setUp()` is often used to assign to attributes used in tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 0 ERRORS: 0 SKIPPED: 0 TOTAL: 2\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # self.data will be set to this value before each test in the TestCase\n",
    "        self.data = {'a': 2} \n",
    "        \n",
    "    def test_foo(self):\n",
    "        # We can test the value of self.data\n",
    "        self.assertEqual(self.data['a'], 2)\n",
    "        \n",
    "        # Modifications to self.data will be reset by setUp()\n",
    "        self.data['a'] = 42\n",
    "        \n",
    "    def test_bar(self):\n",
    "        self.assertEqual(self.data['a'], 2)\n",
    "        self.data['a'] = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `unittest.main()`\n",
    "*Convenience function for running from the command line*\n",
    "\n",
    "Finds all tests in a module and executes them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# unittest_main.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # This fails trivially\n",
    "    def test_foo(self):\n",
    "        self.assertEqual(1, 2)\n",
    "\n",
    "# This the idiomatic test for \"am I the top-level \n",
    "# program\" in Python\n",
    "if __name__ == '__main__':\n",
    "    # If this is the top-level program, this scans \n",
    "    # for all TestCases and runs them.\n",
    "    unittest.main()\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_foo (__main__.Tests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abingham/repos/python-testing-presentation/slides/examples/unittest_main.py\", line 7, in test_foo\n",
      "    self.assertEqual(1, 2)\n",
      "AssertionError: 1 != 2\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "%run -e examples/unittest_main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skipping tests with decorators\n",
    "*Use the decorators `skip()`, `skipIf()`, and `skipUnless()` to skip tests*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 0 ERRORS: 0 SKIPPED: 2 TOTAL: 3\n",
      "s: test_no_run (__main__.Tests)\n",
      "s: test_when_condition_is_false (__main__.Tests)\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "# This controls which tests run\n",
    "cond = True\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    @unittest.skip('This test never runs')\n",
    "    def test_no_run(self):\n",
    "        print('This will never execute in a test suite.')\n",
    "        \n",
    "    # We only run this test if our condition is False\n",
    "    @unittest.skipIf(cond == True, 'Condition is true')\n",
    "    def test_when_condition_is_false(self):\n",
    "        pass\n",
    "    \n",
    "    # We only run this test if our condition is True\n",
    "    @unittest.skipUnless(cond == True, 'Condition is false')\n",
    "    def test_when_condition_is_true(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Skipping tests from fixtures with `SkipTest`\n",
    "* Throw the `SkipTest` exception from `setUp()` to skip tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 0 ERRORS: 0 SKIPPED: 1 TOTAL: 1\n",
      "s: test_contact_satellite (__main__.Tests)\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # We skip tests if a necessary resource is not available\n",
    "        raise unittest.SkipTest(\n",
    "            'Satellite uplink is not available right now.')\n",
    "        \n",
    "    # This test will only be run if the satellite \n",
    "    # uplink is available\n",
    "    def test_contact_satellite(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Allow failures with `expectedFailure`\n",
    "*`expectedFailure` allows a failing test to count as passing*\n",
    "\n",
    "This can be useful for \"reminder\" tests or work-in-progress. For example, it can be used to alert you when a broken upstream dependency has started working.\n",
    "\n",
    "The command-line runner will let you know when there are expected failures *and* when there are unexpected successes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 0 ERRORS: 0 SKIPPED: 0 EXPECTED FAILURES: 1 TOTAL: 1\n",
      "ef: test_this_will_fail (__main__.Tests)\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    # The failing assertion in this test is not counted as a failure\n",
    "    @unittest.expectedFailure\n",
    "    def test_this_will_fail(self):\n",
    "        self.assertTrue(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameterize tests with `TestCase.subTest`\n",
    "## Run a test with multiple times\n",
    "*New in 3.4*\n",
    "\n",
    "This reduces boilerplate in certain situations since failure of one subtest does not prevent execution of other subtests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILURES: 2 ERRORS: 0 SKIPPED: 0 TOTAL: 1\n",
      "f: test_all_ascii (__main__.Tests) (word='tråd')\n",
      "f: test_all_ascii (__main__.Tests) (word='øy')\n"
     ]
    }
   ],
   "source": [
    "%%unittest_run\n",
    "\n",
    "import string\n",
    "\n",
    "class Tests(unittest.TestCase):\n",
    "    def test_all_ascii(self):\n",
    "        for word in ['thread', 'tråd', 'island', 'øy']:\n",
    "            # Each subtest context is treated as an independent test\n",
    "            with self.subTest(word=word):\n",
    "                self.assertTrue(all(c in string.ascii_lowercase \n",
    "                                    for c in word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A complete example\n",
    "## This is a good template `unittest`\n",
    "\n",
    "* Keep tests separate from production code\n",
    "* Use separate `TestCase` subclasses for different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unittest project tree](images/unittest-project-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "# tests/test_sorting.py\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class SortingTests(unittest.TestCase):\n",
    "    def test_basic(self):\n",
    "        self.assertListEqual(sorted([3, 2, 1]), [1, 2, 3])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialisation Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "theme": "simple",
   "transition": "slide"
  },
  "name": "unittest.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
